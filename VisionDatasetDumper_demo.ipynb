{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bbc5b3",
   "metadata": {},
   "source": [
    "# VisionDatasetDumper Demo\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f716d",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf086772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10, SVHN, MNIST, EMNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "from hyperpyper.utils import DataSetDumper, VisionDatasetDumper\n",
    "from hyperpyper.utils import FolderScanner as fs\n",
    "from hyperpyper.transforms import PILTranspose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e52b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.home() / \"Downloads\" / \"data\"\n",
    "\n",
    "DATA_PATH = ROOT_PATH / \"CIFAR10\"\n",
    "\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef93ba",
   "metadata": {},
   "source": [
    "## Create CIFAR10 dataset organized in subfolders indicating class\n",
    "The VisionDatasetDumper handles the download and the creation of a folder structure where images are stored. They can then be used as the starting point for experiments. We only need the dataset returned by the VisionDatasetDumper to extract the class labels to be able to match them with class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d117863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisionDatasetDumper(CIFAR10, root=DATA_PATH, dst=DATA_PATH_TRAIN, train=True).dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed22929",
   "metadata": {},
   "source": [
    "### Retrieve a list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68fb13b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/bernh/Downloads/data/CIFAR10/train/2/18026.png'),\n",
       " WindowsPath('C:/Users/bernh/Downloads/data/CIFAR10/train/6/22439.png'),\n",
       " WindowsPath('C:/Users/bernh/Downloads/data/CIFAR10/train/4/10150.png'),\n",
       " WindowsPath('C:/Users/bernh/Downloads/data/CIFAR10/train/7/15772.png'),\n",
       " WindowsPath('C:/Users/bernh/Downloads/data/CIFAR10/train/7/4650.png')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = fs.get_files(DATA_PATH_TRAIN, extensions='.png', recursive=True)\n",
    "\n",
    "# Select some random items\n",
    "selected_files = random.sample(train_files, 5)\n",
    "selected_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ce922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\bernh\\Downloads\\data\\SVHN\\train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = ROOT_PATH / \"SVHN\"\n",
    "\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")\n",
    "\n",
    "train_dataset = VisionDatasetDumper(SVHN, root=DATA_PATH, dst=DATA_PATH_TRAIN, train=True).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb32fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_PATH / \"MNIST\"\n",
    "\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")\n",
    "\n",
    "train_dataset = VisionDatasetDumper(MNIST, root=DATA_PATH, dst=DATA_PATH_TRAIN, train=True).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea8fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = ROOT_PATH / \"EMNIST\"\n",
    "\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")\n",
    "\n",
    "transform = Compose([\n",
    "    PILTranspose(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = VisionDatasetDumper(EMNIST, root=DATA_PATH, dst=DATA_PATH_TRAIN, split='letters', train=True, transform=transform).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b4bbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 124800\n",
       "    Root location: C:\\Users\\bernh\\Downloads\\data\\EMNIST\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               PILTranspose()\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cab21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
