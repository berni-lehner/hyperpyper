{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bbc5b3",
   "metadata": {},
   "source": [
    "# ImageNet, CIFAR10, and SVHN data and image preview of embedded data points\n",
    "In case anything weird happens, make sure that the caching is used correctly, and nothing is overwritten or misplaced,\n",
    "e.g. if a cache path is shared with other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f716d",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf086772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from random import sample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, SVHN, ImageNet\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Normalize, Resize, CenterCrop\n",
    "\n",
    "from transights.utils import DataSetDumper\n",
    "from transights.utils import FolderScanner as fs\n",
    "from transights.utils import Pickler\n",
    "from transights.utils import EmbeddingPlotter\n",
    "from transights.utils import PipelineCache\n",
    "from transights.transforms import (FileToPIL,\n",
    "                            DummyPIL,\n",
    "                            PILToNumpy,\n",
    "                            FlattenArray,\n",
    "                            FlattenList,\n",
    "                            DebugTransform,\n",
    "                            ProjectTransform,\n",
    "                            PyTorchOutput,\n",
    "                            PyTorchEmbedding,\n",
    "                            ToDevice,\n",
    "                            FlattenTensor,\n",
    "                            CachingTransform,\n",
    "                            TensorToNumpy,\n",
    "                            ToArgMax,\n",
    "                            ToLabel,\n",
    "                            )\n",
    "\n",
    "from transights.aggregator import DataAggregator, DataSetAggregator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "random_state = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b6f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def weights_to_openood_model(weights, model):\n",
    "# Modify the keys to get rid of 'module.' in all the keys\n",
    "    new_state_dict = OrderedDict([(key.replace('module.', ''), value) for key, value in weights.items()])\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4300f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "# this prevents the following error when trying to download the dataset:\n",
    "# SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6b9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CUDA\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43893fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path('D:/')\n",
    "#ROOT_PATH = Path.cwd()\n",
    "DATA_PATH = ROOT_PATH / \"data\"\n",
    "CACHE_PATH = DATA_PATH / \"tmp\"\n",
    "\n",
    "CIFAR10_DATA_PATH = DATA_PATH / \"CIFAR10\"\n",
    "CIFAR10_DATA_PATH_TRAIN = Path(CIFAR10_DATA_PATH, \"train\")\n",
    "CIFAR10_DATA_PATH_TEST = Path(CIFAR10_DATA_PATH, \"test\")\n",
    "\n",
    "SVHN_DATA_PATH = DATA_PATH / \"SVHN\"\n",
    "SVHN_DATA_PATH_TRAIN = Path(SVHN_DATA_PATH, \"train\")\n",
    "SVHN_DATA_PATH_TEST = Path(SVHN_DATA_PATH, \"test\")\n",
    "\n",
    "IMAGENET1K_DATA_PATH = DATA_PATH / \"ImageNet\"\n",
    "IMAGENET1K_DATA_PATH_TRAIN = Path(IMAGENET1K_DATA_PATH, \"train\")\n",
    "# id->class mapping\n",
    "#IMAGENET1K_MAPPING_FILE = Path(\"D:\\data\\imagenet\\imagenet-object-localization-challenge\\LOC_synset_mapping.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b102012",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_train_embedding_pickle_file = Path(CACHE_PATH, \"CIFAR10_train__oodresnet18__embedding.pkl\")\n",
    "CIFAR10_test_embedding_pickle_file = Path(CACHE_PATH, \"CIFAR10_test__oodresnet18__embedding.pkl\")\n",
    "CIFAR10_test_output_pickle_file = Path(CACHE_PATH, \"CIFAR10_test__oodresnet18__output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0931a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN_test_embedding_pickle_file = Path(CACHE_PATH, \"SVHN_test__oodresnet18__embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38281640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file_into_dict(filename):\n",
    "   result_dict = {}\n",
    "   with open(filename, 'r') as file:\n",
    "       for line_number, line in enumerate(file, start=1):\n",
    "           # Split each line into two parts (key and value)\n",
    "           parts = line.strip().split(' ', 1)\n",
    "\n",
    "           # Ensure there are exactly two parts\n",
    "           if len(parts) != 2:\n",
    "               raise ValueError(f\"Error in line {line_number}: Each line must contain exactly two entries.\")\n",
    "\n",
    "           key, value = parts[0], parts[1]\n",
    "           result_dict[key] = value\n",
    "\n",
    "   return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef93ba",
   "metadata": {},
   "source": [
    "## Create CIFAR10 dataset organized in subfolders indicating class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b8f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pretrained = torch.load(\"model.ckpt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bcdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet18_32x32 import ResNet18_32x32 as resnet18\n",
    "\n",
    "# load model with pre-trained weights\n",
    "oodresnet18_model = weights_to_openood_model(weights_pretrained, resnet18(num_classes=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68fb13b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281167"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_files = fs.get_files(IMAGENET1K_DATA_PATH_TRAIN, extensions='.png', recursive=True)#, relative_path=ROOT_PATH)\n",
    "len(IMAGENET1K_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f31970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_files = sample(IMAGENET1K_train_files, 50000)\n",
    "len(IMAGENET1K_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a6e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b53dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_train_dataset = CIFAR10(root=CIFAR10_DATA_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "if not CIFAR10_DATA_PATH_TRAIN.exists():\n",
    "    DataSetDumper(CIFAR10_train_dataset, CIFAR10_DATA_PATH_TRAIN).dump()\n",
    "\n",
    "CIFAR10_test_dataset = CIFAR10(root=CIFAR10_DATA_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "if not CIFAR10_DATA_PATH_TEST.exists():\n",
    "    DataSetDumper(CIFAR10_test_dataset, CIFAR10_DATA_PATH_TEST).dump()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b4cee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_train_files = fs.get_files(CIFAR10_DATA_PATH_TRAIN, extensions='.png', recursive=True)#, relative_path=ROOT_PATH)\n",
    "len(CIFAR10_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae69291e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_test_files = fs.get_files(CIFAR10_DATA_PATH_TEST, extensions='.png', recursive=True)#, relative_path=ROOT_PATH)\n",
    "len(CIFAR10_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72abaaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: D:\\data\\SVHN\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "SVHN_test_dataset = SVHN(root=SVHN_DATA_PATH, split='test', transform=transform, download=True)\n",
    "\n",
    "if not SVHN_DATA_PATH_TEST.exists():\n",
    "    DataSetDumper(SVHN_test_dataset, SVHN_DATA_PATH_TEST).dump(targets=np.unique(SVHN_test_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80d2bce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26032"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVHN_test_files = fs.get_files(SVHN_DATA_PATH_TEST, extensions='.png', recursive=True)#, relative_path=ROOT_PATH)\n",
    "len(SVHN_test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875aa994",
   "metadata": {},
   "source": [
    "## Define Transformation pipeline\n",
    "Notice, that we have a FileToPIL Transformation that handles the loading of the image. This enables us to use the standard Aggregator, where we don't need to take care of a DataSet or DataLoader instantiation.\n",
    "All we need to pass as arguments are a file list and the transformation pipeline, and optionally a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420dd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformation pipeline\n",
    "embedding_pipeline = Compose([\n",
    "    FileToPIL(),\n",
    "    ToTensor(),\n",
    "    CenterCrop(32),\n",
    "    #Resize((32, 32)),\n",
    "    Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ToDevice(DEVICE),\n",
    "    PyTorchEmbedding(oodresnet18_model, device=DEVICE),\n",
    "    ToDevice('cpu'),\n",
    "    FlattenTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db43ec5",
   "metadata": {},
   "source": [
    "## Instantiate Aggregator and extract embeddings\n",
    "The images are stored in the resulting dict as 'item', and the filenames as 'file'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19331866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_result_info(result):\n",
    "    print(f\"data shape: {result['item'].shape}\")\n",
    "    print(f\"target files: {len(result['file'])}\")\n",
    "    print(f\"target sample: {result['file'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1fca57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_in_path(filelist, path_old, path_new):\n",
    "    new_filelist = []\n",
    "    for file in filelist:\n",
    "        file = file.replace(path_old, path_new)\n",
    "        new_filelist.append(file)\n",
    "\n",
    "    return new_filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6022187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([50000, 512])\n",
      "target files: 50000\n",
      "target sample: D:\\data\\ImageNet\\train\\421\\156741.png\n"
     ]
    }
   ],
   "source": [
    "agg = DataAggregator(IMAGENET1K_train_files, transforms=embedding_pipeline, batch_size=320)\n",
    "\n",
    "IMAGENET1K_train_embedding = agg.transform()#cache_file=IMAGENET1K_train_embedding_pickle_file)\n",
    "\n",
    "dump_result_info(IMAGENET1K_train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6737426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([50000, 512])\n",
      "target files: 50000\n",
      "target sample: D:\\data\\CIFAR10\\train\\0\\10008.png\n"
     ]
    }
   ],
   "source": [
    "agg = DataAggregator(CIFAR10_train_files, transforms=embedding_pipeline, batch_size=320)\n",
    "\n",
    "CIFAR10_train_embedding = agg.transform(cache_file=CIFAR10_train_embedding_pickle_file)\n",
    "\n",
    "dump_result_info(CIFAR10_train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e24d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([26032, 512])\n",
      "target files: 26032\n",
      "target sample: D:\\data\\SVHN\\test\\0\\10021.png\n"
     ]
    }
   ],
   "source": [
    "agg = DataAggregator(SVHN_test_files, transforms=embedding_pipeline, batch_size=32)\n",
    "\n",
    "SVHN_test_embedding = agg.transform(cache_file=SVHN_test_embedding_pickle_file)\n",
    "\n",
    "dump_result_info(SVHN_test_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb042852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_str = \"F:\\\\\"\n",
    "find_str = \"\"\n",
    "replace_str = \"D:\\\\\"\n",
    "\n",
    "if find_str:\n",
    "    IMAGENET1K_train_embedding['file'] = replace_in_path(IMAGENET1K_train_embedding['file'], find_str, replace_str)\n",
    "    CIFAR10_train_embedding['file'] = replace_in_path(CIFAR10_train_embedding['file'], find_str, replace_str)\n",
    "    SVHN_test_embedding['file'] = replace_in_path(SVHN_test_embedding['file'], find_str, replace_str)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1596a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ee0c94a",
   "metadata": {},
   "source": [
    "## Instantiate Aggregator and extract outputs\n",
    "We want to know the performance of the model on CIFAR10 test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f949ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformation pipeline\n",
    "output_pipeline = Compose([\n",
    "    FileToPIL(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ToDevice(DEVICE),\n",
    "    PyTorchOutput(oodresnet18_model, device=DEVICE),\n",
    "    ToDevice('cpu'),\n",
    "    ToArgMax(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47983145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([10000])\n",
      "target files: 10000\n",
      "target sample: D:\\data\\CIFAR10\\test\\0\\10.png\n"
     ]
    }
   ],
   "source": [
    "agg = DataAggregator(CIFAR10_test_files, transforms=output_pipeline, batch_size=320)\n",
    "\n",
    "CIFAR10_test_output = agg.transform(cache_file=CIFAR10_test_output_pickle_file)\n",
    "\n",
    "dump_result_info(CIFAR10_test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dab62de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "CIFAR10_test_y = [Path(file).parts[-2] for file in CIFAR10_test_output['file']]\n",
    "CIFAR10_test_y = np.array(CIFAR10_test_y).astype('int64')\n",
    "np.unique(CIFAR10_test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca79f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_test_output['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f98edd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.084\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_test_pred_y = CIFAR10_test_output['item']\n",
    "CIFAR10_test_accuracy = accuracy_score(CIFAR10_test_y, CIFAR10_test_pred_y)\n",
    "\n",
    "print(f\"Test Accuracy: {CIFAR10_test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aefb3599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 5, 6, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(CIFAR10_test_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3f62862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Create the UMAP reducer instance\n",
    "reducer = UMAP(n_neighbors=25, # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n",
    "               n_components=2, # default 2, The dimension of the space to embed into.\n",
    "               metric='euclidean', # default 'euclidean', The metric to use to compute distances in high dimensional space.\n",
    "               n_epochs=1000, # default None, The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. \n",
    "               learning_rate=1.0, # default 1.0, The initial learning rate for the embedding optimization.\n",
    "               init='spectral', # default 'spectral', How to initialize the low dimensional embedding. Options are: {'spectral', 'random', A numpy array of initial embedding positions}.\n",
    "               min_dist=0.1, # default 0.1, The effective minimum distance between embedded points.\n",
    "               spread=1.0, # default 1.0, The effective scale of embedded points. In combination with ``min_dist`` this determines how clustered/clumped the embedded points are.\n",
    "               low_memory=False, # default False, For some datasets the nearest neighbor computation can consume a lot of memory. If you find that UMAP is failing due to memory constraints consider setting this option to True.\n",
    "               set_op_mix_ratio=1.0, # default 1.0, The value of this parameter should be between 0.0 and 1.0; a value of 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy intersection.\n",
    "               local_connectivity=1, # default 1, The local connectivity required -- i.e. the number of nearest neighbors that should be assumed to be connected at a local level.\n",
    "               repulsion_strength=1.0, # default 1.0, Weighting applied to negative samples in low dimensional embedding optimization.\n",
    "               negative_sample_rate=5, # default 5, Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy.\n",
    "               transform_queue_size=4.0, # default 4.0, Larger values will result in slower performance but more accurate nearest neighbor evaluation.\n",
    "               a=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               b=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               random_state=random_state, # default: None, If int, random_state is the seed used by the random number generator;\n",
    "               metric_kwds=None, # default None) Arguments to pass on to the metric, such as the ``p`` value for Minkowski distance.\n",
    "               angular_rp_forest=False, # default False, Whether to use an angular random projection forest to initialise the approximate nearest neighbor search.\n",
    "               target_n_neighbors=-1, # default -1, The number of nearest neighbors to use to construct the target simplcial set. If set to -1 use the ``n_neighbors`` value.\n",
    "               #target_metric='categorical', # default 'categorical', The metric used to measure distance for a target array is using supervised dimension reduction. By default this is 'categorical' which will measure distance in terms of whether categories match or are different. \n",
    "               #target_metric_kwds=None, # dict, default None, Keyword argument to pass to the target metric when performing supervised dimension reduction. If None then no arguments are passed on.\n",
    "               #target_weight=0.5, # default 0.5, weighting factor between data topology and target topology.\n",
    "               transform_seed=42, # default 42, Random seed used for the stochastic aspects of the transform operation.\n",
    "               verbose=False, # default False, Controls verbosity of logging.\n",
    "               unique=False, # default False, Controls if the rows of your data should be uniqued before being embedded. \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70906229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_CIFAR_train_X = np.vstack((IMAGENET1K_train_embedding['item'], CIFAR10_train_embedding['item']))\n",
    "IMAGENET1K_train_CIFAR_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c265d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('umap', reducer),\n",
    "])\n",
    "\n",
    "#UMAP_2D_CACHE_PATH = CACHE_PATH / \"umap2d\"\n",
    "#pipeline = PipelineCache(pipeline, cache_path=UMAP_2D_CACHE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "219eefd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;umap&#x27;,\n",
       "                 UMAP(local_connectivity=1, low_memory=False, n_epochs=1000, n_neighbors=25, random_state=23, tqdm_kwds={&#x27;bar_format&#x27;: &#x27;{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]&#x27;, &#x27;desc&#x27;: &#x27;Epochs completed&#x27;, &#x27;disable&#x27;: True}))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;umap&#x27;,\n",
       "                 UMAP(local_connectivity=1, low_memory=False, n_epochs=1000, n_neighbors=25, random_state=23, tqdm_kwds={&#x27;bar_format&#x27;: &#x27;{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]&#x27;, &#x27;desc&#x27;: &#x27;Epochs completed&#x27;, &#x27;disable&#x27;: True}))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">UMAP</label><div class=\"sk-toggleable__content\"><pre>UMAP(local_connectivity=1, low_memory=False, n_epochs=1000, n_neighbors=25, random_state=23, tqdm_kwds={&#x27;bar_format&#x27;: &#x27;{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]&#x27;, &#x27;desc&#x27;: &#x27;Epochs completed&#x27;, &#x27;disable&#x27;: True})</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('umap',\n",
       "                 UMAP(local_connectivity=1, low_memory=False, n_epochs=1000, n_neighbors=25, random_state=23, tqdm_kwds={'bar_format': '{desc}: {percentage:3.0f}%| {bar} {n_fmt}/{total_fmt} [{elapsed}]', 'desc': 'Epochs completed', 'disable': True}))])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(IMAGENET1K_train_CIFAR_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d0babc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_reduced_embedding = pipeline.transform(IMAGENET1K_train_embedding['item'])\n",
    "IMAGENET1K_train_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "864cf3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_train_reduced_embedding = pipeline.transform(CIFAR10_train_embedding['item'])\n",
    "CIFAR10_train_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d883cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26032, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVHN_test_reduced_embedding = pipeline.transform(SVHN_test_embedding['item'])\n",
    "SVHN_test_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "743bace6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126032, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_CIFAR_train_SVHN_test_reduced_embedding = np.vstack((IMAGENET1K_train_reduced_embedding, CIFAR10_train_reduced_embedding, SVHN_test_reduced_embedding))\n",
    "IMAGENET1K_train_CIFAR_train_SVHN_test_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a2a88f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126032"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET1K_train_CIFAR_train_SVHN_test__file_list = IMAGENET1K_train_embedding['file'] + CIFAR10_train_embedding['file'] + SVHN_test_embedding['file']\n",
    "len(IMAGENET1K_train_CIFAR_train_SVHN_test__file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd93fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET1K_train_y_str = [\"IMAGENET1K\"] * len(IMAGENET1K_train_embedding['item'])\n",
    "CIFAR10_train_y_str = [\"CIFAR10\"] * len(CIFAR10_train_embedding['item'])\n",
    "SVHN_test_y_str = [\"SVHN\"] * len(SVHN_test_embedding['item'])\n",
    "\n",
    "#IMAGENET1K_train__CIFAR10_train_y_str = IMAGENET1K_train_y_str + CIFAR10_train_y_str\n",
    "#len(IMAGENET1K_train__CIFAR10_train_y_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf9ad461",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_train_y = [Path(file).parts[-2] for file in CIFAR10_train_embedding['file']]\n",
    "CIFAR10_train_y = np.array(CIFAR10_train_y).astype('int')\n",
    "\n",
    "# Map class index to label\n",
    "CIFAR10_labels = CIFAR10_train_dataset.classes\n",
    "\n",
    "CIFAR10_train_y_str = [\"CIFAR10_train_\" + CIFAR10_labels[i] for i in CIFAR10_train_y]\n",
    "CIFAR10_test_y_str = [\"CIFAR10_test_\" + CIFAR10_labels[i] for i in CIFAR10_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48dc4dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126032"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMAGENET1K_train__CIFAR10_train__SVHN_test__y_str = IMAGENET1K_train_y_str + CIFAR10_train_y_str + SVHN_test_y_str\n",
    "IMAGENET1K_train__CIFAR10_train__SVHN_test__y_str = IMAGENET1K_train_y_str + CIFAR10_train_y_str + SVHN_test_y_str\n",
    "len(IMAGENET1K_train__CIFAR10_train__SVHN_test__y_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "651c8154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3a2534f7b74d01b690cda16875a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(FigureWidget({\n",
       "    'data': [{'hovertemplate': '<b>%{hovertext}</b><br><br>color=IMAGENET1K<br>x=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = EmbeddingPlotter(data=IMAGENET1K_train_CIFAR_train_SVHN_test_reduced_embedding,\n",
    "                           color=IMAGENET1K_train__CIFAR10_train__SVHN_test__y_str,\n",
    "                           #color=CIFAR10_SVHN_full_str,\n",
    "                           file_list=IMAGENET1K_train_CIFAR_train_SVHN_test__file_list,\n",
    "                           hover_name=IMAGENET1K_train_CIFAR_train_SVHN_test__file_list,\n",
    "                           width=1000)\n",
    "\n",
    "display(plotter.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "366c283f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
