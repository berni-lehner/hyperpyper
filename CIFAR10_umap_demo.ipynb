{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bbc5b3",
   "metadata": {},
   "source": [
    "# Demo with CIFAR10 data and image preview of embedded data points\n",
    "For this, we need to have CIFAR10 data organized in subfolders (one for each class). We can then use the standard Aggregator, which returns an image and the corresponding filename.\n",
    "We use the image data to extract the embeddings from a pretrained Resnet, and reduce the dimensionality further with UMAP down to just 2 dimensions.\n",
    "Then, we use a scatter plot that additionally plots the corresponding images when we hover over a data point with the mouse pointer.\n",
    "We do this only with the test data, since it is smaller and the notebook will execute faster, but you can easily do the same with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f716d",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf086772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Normalize, Resize\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "from transights.utils import DataSetDumper\n",
    "from transights.utils import FolderScanner as fs\n",
    "from transights.utils import Pickler\n",
    "from transights.utils import EmbeddingPlotter\n",
    "from transights.transforms import (FileToPIL,\n",
    "                            DummyPIL,\n",
    "                            PILToNumpy,\n",
    "                            FlattenArray,\n",
    "                            DebugTransform,\n",
    "                            ProjectTransform,\n",
    "                            PyTorchOutput,\n",
    "                            PyTorchEmbedding,\n",
    "                            ToDevice,\n",
    "                            FlattenTensor,\n",
    "                            CachingTransform)\n",
    "\n",
    "from transights.aggregator import DataAggregator, DataSetAggregator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "random_state = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.home() / \"Downloads\"\n",
    "\n",
    "DATA_PATH = ROOT_PATH / \"data\" / \"CIFAR10\"\n",
    "\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "# this prevents the following error when trying to download the dataset:\n",
    "# SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef93ba",
   "metadata": {},
   "source": [
    "## Create CIFAR10 dataset organized in subfolders indicating class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54876fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR10(root=DATA_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "if not DATA_PATH_TRAIN.exists():\n",
    "    DataSetDumper(train_dataset, DATA_PATH_TRAIN).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c676a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CIFAR10(root=DATA_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "if not DATA_PATH_TEST.exists():\n",
    "    DataSetDumper(test_dataset, DATA_PATH_TEST).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pretrained = torch.load(\"weights_resnet18_cifar10.pth\", map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = fs.get_files(DATA_PATH_TRAIN, extensions='.png', recursive=True)\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = fs.get_files(DATA_PATH_TEST, extensions='.png', recursive=True)\n",
    "len(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875aa994",
   "metadata": {},
   "source": [
    "## Define Transformation pipeline\n",
    "Notice, that we have a FileToPIL Transformation that handles the loading of the image. This enables us to use the standard Aggregator, where we don't need to take care of a DataSet or DataLoader instantiation.\n",
    "All we need to pass as arguments are a file list and the transformation pipeline, and optionally a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformation pipeline\n",
    "transform_pipeline = Compose([\n",
    "    FileToPIL(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ToDevice(DEVICE),\n",
    "    PyTorchEmbedding(model, device=DEVICE),\n",
    "    ToDevice('cpu'),\n",
    "    FlattenTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db43ec5",
   "metadata": {},
   "source": [
    "## Instantiate Aggregator and extract embeddings\n",
    "The images are stored in the resulting dict as 'item', and the filenames as 'file'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = DataAggregator(test_files, transforms=transform_pipeline, batch_size=32)\n",
    "\n",
    "test_embedding_result = agg.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19331866",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_result['item'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_result['file'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9315f29",
   "metadata": {},
   "source": [
    "# Create 2D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f62862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Create the UMAP reducer instance\n",
    "reducer = UMAP(n_neighbors=15, # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n",
    "               n_components=2, # default 2, The dimension of the space to embed into.\n",
    "               metric='euclidean', # default 'euclidean', The metric to use to compute distances in high dimensional space.\n",
    "               n_epochs=1000, # default None, The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. \n",
    "               learning_rate=1.0, # default 1.0, The initial learning rate for the embedding optimization.\n",
    "               init='spectral', # default 'spectral', How to initialize the low dimensional embedding. Options are: {'spectral', 'random', A numpy array of initial embedding positions}.\n",
    "               min_dist=0.1, # default 0.1, The effective minimum distance between embedded points.\n",
    "               spread=1.0, # default 1.0, The effective scale of embedded points. In combination with ``min_dist`` this determines how clustered/clumped the embedded points are.\n",
    "               low_memory=False, # default False, For some datasets the nearest neighbor computation can consume a lot of memory. If you find that UMAP is failing due to memory constraints consider setting this option to True.\n",
    "               set_op_mix_ratio=1.0, # default 1.0, The value of this parameter should be between 0.0 and 1.0; a value of 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy intersection.\n",
    "               local_connectivity=1, # default 1, The local connectivity required -- i.e. the number of nearest neighbors that should be assumed to be connected at a local level.\n",
    "               repulsion_strength=1.0, # default 1.0, Weighting applied to negative samples in low dimensional embedding optimization.\n",
    "               negative_sample_rate=5, # default 5, Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy.\n",
    "               transform_queue_size=4.0, # default 4.0, Larger values will result in slower performance but more accurate nearest neighbor evaluation.\n",
    "               a=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               b=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               random_state=random_state, # default: None, If int, random_state is the seed used by the random number generator;\n",
    "               metric_kwds=None, # default None) Arguments to pass on to the metric, such as the ``p`` value for Minkowski distance.\n",
    "               angular_rp_forest=False, # default False, Whether to use an angular random projection forest to initialise the approximate nearest neighbor search.\n",
    "               target_n_neighbors=-1, # default -1, The number of nearest neighbors to use to construct the target simplcial set. If set to -1 use the ``n_neighbors`` value.\n",
    "               #target_metric='categorical', # default 'categorical', The metric used to measure distance for a target array is using supervised dimension reduction. By default this is 'categorical' which will measure distance in terms of whether categories match or are different. \n",
    "               #target_metric_kwds=None, # dict, default None, Keyword argument to pass to the target metric when performing supervised dimension reduction. If None then no arguments are passed on.\n",
    "               #target_weight=0.5, # default 0.5, weighting factor between data topology and target topology.\n",
    "               transform_seed=42, # default 42, Random seed used for the stochastic aspects of the transform operation.\n",
    "               verbose=False, # default False, Controls verbosity of logging.\n",
    "               unique=False, # default False, Controls if the rows of your data should be uniqued before being embedded. \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c265d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('umap', reducer),\n",
    "])\n",
    "\n",
    "\n",
    "X = test_embedding_result['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced_embedding = pipeline.transform(X)\n",
    "test_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2725909",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [Path(file).parts[-2] for file in test_embedding_result['file']]\n",
    "test_y = np.array(test_y).astype('int')\n",
    "\n",
    "# Map class index to label\n",
    "labels = test_dataset.classes\n",
    "test_y_str = [labels[i] for i in test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb478c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f0cb4",
   "metadata": {},
   "source": [
    "# Create 2D Plot\n",
    "Just pass a 2d array to the EmbeddingPlotter, it will automatically plot accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = EmbeddingPlotter(data=test_reduced_embedding,\n",
    "                           color=test_y_str,\n",
    "                           file_list=test_embedding_result['file'],\n",
    "                           hover_name=test_embedding_result['file'],\n",
    "                           width=1000)\n",
    "display(plotter.plot())                           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f98a28",
   "metadata": {},
   "source": [
    "# Create 2D Density Plot\n",
    "Just pass 'kde' as color parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = EmbeddingPlotter(data=test_reduced_embedding,\n",
    "                           color='kde',\n",
    "                           file_list=test_embedding_result['file'],\n",
    "                           hover_name=test_embedding_result['file'],\n",
    "                           width=1000)\n",
    "display(plotter.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72c5fd",
   "metadata": {},
   "source": [
    "# Create 3D Plot\n",
    "Just pass a 3d array to the EmbeddingPlotter, it will automatically plot accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Create the UMAP reducer instance\n",
    "reducer = UMAP(n_neighbors=15, # default 15, The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n",
    "               n_components=3, # default 2, The dimension of the space to embed into.\n",
    "               metric='euclidean', # default 'euclidean', The metric to use to compute distances in high dimensional space.\n",
    "               n_epochs=1000, # default None, The number of training epochs to be used in optimizing the low dimensional embedding. Larger values result in more accurate embeddings. \n",
    "               learning_rate=1.0, # default 1.0, The initial learning rate for the embedding optimization.\n",
    "               init='spectral', # default 'spectral', How to initialize the low dimensional embedding. Options are: {'spectral', 'random', A numpy array of initial embedding positions}.\n",
    "               min_dist=0.1, # default 0.1, The effective minimum distance between embedded points.\n",
    "               spread=1.0, # default 1.0, The effective scale of embedded points. In combination with ``min_dist`` this determines how clustered/clumped the embedded points are.\n",
    "               low_memory=False, # default False, For some datasets the nearest neighbor computation can consume a lot of memory. If you find that UMAP is failing due to memory constraints consider setting this option to True.\n",
    "               set_op_mix_ratio=1.0, # default 1.0, The value of this parameter should be between 0.0 and 1.0; a value of 1.0 will use a pure fuzzy union, while 0.0 will use a pure fuzzy intersection.\n",
    "               local_connectivity=1, # default 1, The local connectivity required -- i.e. the number of nearest neighbors that should be assumed to be connected at a local level.\n",
    "               repulsion_strength=1.0, # default 1.0, Weighting applied to negative samples in low dimensional embedding optimization.\n",
    "               negative_sample_rate=5, # default 5, Increasing this value will result in greater repulsive force being applied, greater optimization cost, but slightly more accuracy.\n",
    "               transform_queue_size=4.0, # default 4.0, Larger values will result in slower performance but more accurate nearest neighbor evaluation.\n",
    "               a=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               b=None, # default None, More specific parameters controlling the embedding. If None these values are set automatically as determined by ``min_dist`` and ``spread``.\n",
    "               random_state=random_state, # default: None, If int, random_state is the seed used by the random number generator;\n",
    "               metric_kwds=None, # default None) Arguments to pass on to the metric, such as the ``p`` value for Minkowski distance.\n",
    "               angular_rp_forest=False, # default False, Whether to use an angular random projection forest to initialise the approximate nearest neighbor search.\n",
    "               target_n_neighbors=-1, # default -1, The number of nearest neighbors to use to construct the target simplcial set. If set to -1 use the ``n_neighbors`` value.\n",
    "               #target_metric='categorical', # default 'categorical', The metric used to measure distance for a target array is using supervised dimension reduction. By default this is 'categorical' which will measure distance in terms of whether categories match or are different. \n",
    "               #target_metric_kwds=None, # dict, default None, Keyword argument to pass to the target metric when performing supervised dimension reduction. If None then no arguments are passed on.\n",
    "               #target_weight=0.5, # default 0.5, weighting factor between data topology and target topology.\n",
    "               transform_seed=42, # default 42, Random seed used for the stochastic aspects of the transform operation.\n",
    "               verbose=False, # default False, Controls verbosity of logging.\n",
    "               unique=False, # default False, Controls if the rows of your data should be uniqued before being embedded. \n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('umap', reducer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced_embedding = pipeline.transform(X)\n",
    "test_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = EmbeddingPlotter(data=test_reduced_embedding,\n",
    "                           color=test_y_str,\n",
    "                           file_list=test_embedding_result['file'],\n",
    "                           hover_name=test_embedding_result['file'],\n",
    "                           width=1000)\n",
    "display(plotter.plot())                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d386c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = EmbeddingPlotter(data=test_reduced_embedding,\n",
    "                           color='kde',\n",
    "                           file_list=test_embedding_result['file'],\n",
    "                           hover_name=test_embedding_result['file'],\n",
    "                           width=1000)\n",
    "display(plotter.plot())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
