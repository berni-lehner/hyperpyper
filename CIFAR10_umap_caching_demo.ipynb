{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bbc5b3",
   "metadata": {},
   "source": [
    "# Demo with CIFAR10 data and image preview of embedded data points\n",
    "For this, we need to have CIFAR10 data organized in subfolders (one for each class). We can then use the standard Aggregator, which returns an image and the corresponding filename.\n",
    "We use the image data to extract the embeddings from a pretrained Resnet, and reduce the dimensionality further with UMAP down to just 2 dimensions.\n",
    "Then, we use a scatter plot that additionally plots the corresponding images when we hover over a data point with the mouse pointer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f716d",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf086772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, Normalize, Resize\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from hyperpyper.utils import IndexToClassLabelDecoder, ClassToIndexLabelDecoder, FileToClassLabelDecoder\n",
    "from hyperpyper.utils import DataSetDumper, VisionDatasetDumper\n",
    "from hyperpyper.utils import FolderScanner as fs\n",
    "from hyperpyper.utils import Pickler\n",
    "from hyperpyper.utils import EmbeddingPlotter\n",
    "from hyperpyper.utils import PipelineCache\n",
    "from hyperpyper.utils import PathList\n",
    "from hyperpyper.transforms import (FileToPIL,\n",
    "                            DummyPIL,\n",
    "                            PILToNumpy,\n",
    "                            FlattenArray,\n",
    "                            DebugTransform,\n",
    "                            ProjectTransform,\n",
    "                            PyTorchOutput,\n",
    "                            PyTorchEmbedding,\n",
    "                            ToDevice,\n",
    "                            FlattenTensor,\n",
    "                            CachingTransform)\n",
    "from hyperpyper.aggregator import DataAggregator, DataSetAggregator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "random_state = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6b9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: CUDA\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on device:\", DEVICE.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e52b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.home() / \"Downloads\" / \"data\"\n",
    "\n",
    "DATA_PATH = ROOT_PATH / \"CIFAR10\"\n",
    "\n",
    "DATA_PATH_TEST = Path(DATA_PATH, \"test\")\n",
    "DATA_PATH_TRAIN = Path(DATA_PATH, \"train\")\n",
    "\n",
    "CACHE_PATH = DATA_PATH / \"tmp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d34d4",
   "metadata": {},
   "source": [
    "### Define a folder for UMAP specific cache, a filename is automatically generated based on the data that is processed with the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e690d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_2D_CACHE_PATH = CACHE_PATH / \"umap2d\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a5ceb",
   "metadata": {},
   "source": [
    "### Define a file for the caching of the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d019ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_train_embedding_resnet18_file = Path(CACHE_PATH, \"CIFAR10_train_embedding_resnet18.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82224f1f",
   "metadata": {},
   "source": [
    "### Let's see if there are any cache files present already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25cdf68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_file = fs.get_files(CACHE_PATH, recursive=True, relative_to=CACHE_PATH)\n",
    "cache_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef93ba",
   "metadata": {},
   "source": [
    "## Create CIFAR10 dataset organized in subfolders indicating class\n",
    "The VisionDatasetDumper handles the download and the creation of a folder structure where images are stored. They can then be used as the starting point for experiments. We only need the dataset returned by the VisionDatasetDumper to extract the class labels to be able to match them with class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d117863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = VisionDatasetDumper(CIFAR10, root=DATA_PATH, dst=DATA_PATH_TRAIN, train=True).dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d27e239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed22929",
   "metadata": {},
   "source": [
    "### Retrieve a list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68fb13b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = fs.get_files(DATA_PATH_TRAIN, extensions='.png', recursive=True)\n",
    "\n",
    "len(train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69767cbc",
   "metadata": {},
   "source": [
    "### Load a pre-trained PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7b8f6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_pretrained = torch.load(\"weights_resnet18_cifar10.pth\", map_location=DEVICE)\n",
    "\n",
    "# load model with pre-trained weights\n",
    "model = resnet18(num_classes=10)\n",
    "model.load_state_dict(weights_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875aa994",
   "metadata": {},
   "source": [
    "## Define Transformation pipeline\n",
    "Notice, that we have a FileToPIL Transformation that handles the loading of the image. This enables us to use the standard Aggregator, where we don't need to take care of a DataSet or DataLoader instantiation.\n",
    "All we need to pass as arguments are a file list and the transformation pipeline, and optionally a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "420dd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transformation pipeline\n",
    "transform_pipeline = Compose([\n",
    "    FileToPIL(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ToDevice(DEVICE),\n",
    "    PyTorchEmbedding(model, device=DEVICE),\n",
    "    ToDevice('cpu'),\n",
    "    FlattenTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db43ec5",
   "metadata": {},
   "source": [
    "## Instantiate Aggregator and extract embeddings\n",
    "The result follows the torchvision standard of being (X,y) tuples. Notice, y corresponds to files, and not to a target. The reason for this is that we want to keep data and the corresponding files together, even in case the mini batch procedure shuffles the order. The files are later used to connect a UMAP data point to the corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15512f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = DataAggregator(files=train_files, transforms=transform_pipeline, batch_size=320)\n",
    "\n",
    "train_X, train_y_files = agg.transform(cache_file=CIFAR10_train_embedding_resnet18_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b215b5b",
   "metadata": {},
   "source": [
    "### Since transform() has been executed, a cache file has been created (in case it was not already there)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72aaf08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('CIFAR10_train_embedding_resnet18.pkl')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_file = fs.get_files(CACHE_PATH, recursive=True, relative_to=CACHE_PATH)\n",
    "cache_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f62862",
   "metadata": {},
   "source": [
    "### Define the pipeline to feed the embedding vectors to a UMAP dimensionality reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c265d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('umap', UMAP()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080c0aa",
   "metadata": {},
   "source": [
    "### Wrap a PipelineCache around the pipeline to automatically cache the output of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d291d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PipelineCache(pipeline, cache_path=UMAP_2D_CACHE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fe1955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced_embedding = pipeline.fit_transform(train_X)\n",
    "train_reduced_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d8f3c",
   "metadata": {},
   "source": [
    "### After fit(), transform(), or fit_transform(), a cache file is created that corresponds to the data processed with the pipeline\n",
    "Each function will yield a corresponding subfolder, where the cache file is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ef8dfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('fit_transform/98ffc4140f08d25694767acac014fa795b6d341e3944c1faa26f378169d22a10.pkl')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_file = fs.get_files(UMAP_2D_CACHE_PATH, recursive=True, relative_to=UMAP_2D_CACHE_PATH)\n",
    "cache_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1a725",
   "metadata": {},
   "source": [
    "### Translate indices to class names and vice versa\n",
    "Sklearn has encoders/decoders as well, but here we want to use the folder names to infer the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a2ab9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class indices from filenames\n",
    "file_encoder = FileToClassLabelDecoder()\n",
    "train_y = file_encoder(train_y_files)\n",
    "\n",
    "# Convert indices to class labels\n",
    "label_decoder = IndexToClassLabelDecoder(train_dataset.classes)\n",
    "train_y_str = label_decoder(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9650c",
   "metadata": {},
   "source": [
    "### Let's have a look at the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca13f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'airplane': 5000,\n",
       "         'automobile': 5000,\n",
       "         'bird': 5000,\n",
       "         'cat': 5000,\n",
       "         'deer': 5000,\n",
       "         'dog': 5000,\n",
       "         'frog': 5000,\n",
       "         'horse': 5000,\n",
       "         'ship': 5000,\n",
       "         'truck': 5000})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter(train_y_str)\n",
    "ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fdb2c",
   "metadata": {},
   "source": [
    "## Plot the UMAP dimensionality reduced embedding vectors and an image preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "651c8154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990bd63b5fed43bc91f7ff41c8abca61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(FigureWidget({\n",
       "    'data': [{'hovertemplate': '<b>%{hovertext}</b><br><br>color=airplane<br>x=%{â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = EmbeddingPlotter(data=train_reduced_embedding,\n",
    "                           color=train_y_str,\n",
    "                           file_list=train_y_files,\n",
    "                           width=1000)\n",
    "display(plotter.plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c941d",
   "metadata": {},
   "source": [
    "## A second run of the notebook will be much faster, as it benefits from the cached results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
